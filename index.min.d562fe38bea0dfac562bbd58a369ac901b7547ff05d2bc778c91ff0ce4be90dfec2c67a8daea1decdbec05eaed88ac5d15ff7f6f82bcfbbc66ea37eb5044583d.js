var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/gdg/docs/releases/",title:"Releases",description:"",content:""}),e.add({id:1,href:"/gdg/docs/gdg/",title:"GDG Docs",description:"",content:""}),e.add({id:2,href:"/gdg/docs/gdg/installation/",title:"Installation",description:`Installation # The easiest way to install GDG is to get one of the pre-compiled binaries from our release page which can be found here. Packages for a few distributions have been added. The release cycle relies on goreleaser so anything that is well supported can be added it at some point. There is no APT or such you can connect to but the packages are available for download.
The following packages are currently supported:`,content:`Installation # The easiest way to install GDG is to get one of the pre-compiled binaries from our release page which can be found here. Packages for a few distributions have been added. The release cycle relies on goreleaser so anything that is well supported can be added it at some point. There is no APT or such you can connect to but the packages are available for download.
The following packages are currently supported:
RPM APK Docker Package Installation # Install from package involves downloading the appropriate package from the release and installing it as you usually do on your favorite Distro.
rpm -Uvh ./gdg_0.3.1_amd64.rpm dpkg -i ./gdg_0.3.1_amd64.deb Docker usage # The docker tags are released started with 0.3.1. Each release will generate a major version and minor version tag.
You can see the available images here
docker pull ghcr.io/esnet/gdg:0.3.1 NOTE: ghcr.io/esnet/gdg:0.3 will also point to 0.3.1 until 0.3.2 is released after which it\u0026rsquo;ll point to 0.3.2
Example compose.
version: '3.7' services: gdg: image: ghcr.io/esnet/gdg:0.3.1 command: \u0026quot;--help\u0026quot; ## Add additional parameters here # command: \u0026quot;ds export\u0026quot; ## Pass any commands on here. volumes: - ./conf:/app/conf ## where the configuration lives - ./exports:/app/exports ## doesn't need to be /app/exports but you should export the destination of where exports are being written out to. From the CLI:
docker run -it --rm -v $(pwd)/conf:/app/conf -v $(pwd)/exports:/app/exports ghcr.io/esnet/gdg:latest ds --help Installing via Go # If you have go install you may run the following command to install gdg
go install github.com/esnet/gdg@latest #for latest go install github.com/esnet/gdg@v0.3.1 #for a specific version You can verify the version by running gdg version.
Configuration # You can then create a simple configuration using gdg ctx new which will do a best effort to guide to setup a basic config that will get you up and going or read the more detailed documentation that can be found here
NOTE: wizard doesn\u0026rsquo;t currently support ALL features but it should help you get a head start.
`}),e.add({id:3,href:"/gdg/docs/tutorials/",title:"Tutorials",description:"",content:""}),e.add({id:4,href:"/gdg/docs/tutorials/library_elements/",title:"Working with Library Panels",description:`Starting with version 0.4, library panels are going to be supported. It\u0026rsquo;s a bit special and the behavior is somewhat unique.
Rules:
Library Panels are immutable. They cannot be moved to a different folder. They are linked to one or multiple dashboards. The only way I can see to move a lib element is to unlink the panel, delete the panel and re-create it in a different folder, then re-link it.`,content:`Starting with version 0.4, library panels are going to be supported. It\u0026rsquo;s a bit special and the behavior is somewhat unique.
Rules:
Library Panels are immutable. They cannot be moved to a different folder. They are linked to one or multiple dashboards. The only way I can see to move a lib element is to unlink the panel, delete the panel and re-create it in a different folder, then re-link it. In theory it\u0026rsquo;s supposed to move with the dashboards but I haven\u0026rsquo;t been able to re-create that behavior. You cannot delete a library element while a dashboard is still using it. Import components # will retrieve all the components from Grafana and save to local file system.
gdg lib download ┌─────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ TYPE │ FILENAME │ ├─────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────┤ │ library │ testing_data/libraryelements/General/dashboard-makeover-extra-cleaning-duty-assignment-today.json │ │ library │ testing_data/libraryelements/General/dashboard-makeover-lighting-status.json │ │ library │ testing_data/libraryelements/General/dashboard-makeover-side-dish-prep-times-past-7-days.json │ │ library │ testing_data/libraryelements/General/dashboard-makeover-time-since-we-purchased-these-spices.json │ │ library │ testing_data/libraryelements/General/extreme-dashboard-makeover-grill.json │ │ library │ testing_data/libraryelements/General/extreme-dashboard-makeover-mac-oven.json │ │ library │ testing_data/libraryelements/General/extreme-dashboard-makeover-refrigerator-temperature-f.json │ │ library │ testing_data/libraryelements/General/extreme-dashboard-makeover-room-temperature-f.json │ │ library │ testing_data/libraryelements/General/extreme-dashboard-makeover-salmon-cooking-times-past-7-days.json │ └─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────┘ Importing Dashboards # Now that we the library components, pulled let\u0026rsquo;s pull the Dashboard.
gdg dash download INFO[0002] Importing dashboards for context: 'local' ┌───────────┬───────────────────────────────────────────────────────────────────┐ │ TYPE │ FILENAME │ ├───────────┼───────────────────────────────────────────────────────────────────┤ │ dashboard │ testing_data/dashboards/General/bandwidth-dashboard.json │ │ dashboard │ testing_data/dashboards/General/bandwidth-patterns.json │ │ dashboard │ testing_data/dashboards/Other/dashboard-makeover-challenge.json │ \u0026lt;== uses library panels │ dashboard │ testing_data/dashboards/Other/flow-analysis.json │ │ dashboard │ testing_data/dashboards/Other/flow-data-for-circuits.json │ │ dashboard │ testing_data/dashboards/Other/flow-data-for-projects.json │ │ dashboard │ testing_data/dashboards/Other/flow-data-per-country.json │ │ dashboard │ testing_data/dashboards/Other/flow-data-per-organization.json │ │ dashboard │ testing_data/dashboards/Other/flow-information.json │ │ dashboard │ testing_data/dashboards/Other/flows-by-science-discipline.json │ │ dashboard │ testing_data/dashboards/General/individual-flows.json │ │ dashboard │ testing_data/dashboards/General/individual-flows-per-country.json │ │ dashboard │ testing_data/dashboards/Ignored/latency-patterns.json │ │ dashboard │ testing_data/dashboards/General/loss-patterns.json │ │ dashboard │ testing_data/dashboards/General/other-flow-stats.json │ │ dashboard │ testing_data/dashboards/General/science-discipline-patterns.json │ │ dashboard │ testing_data/dashboards/General/top-talkers-over-time.json │ └───────────┴───────────────────────────────────────────────────────────────────┘ The dashboards will have a reference to the library panel linked by UID.
Here\u0026rsquo;s the json from the dashboard JSON:
\u0026quot;libraryPanel\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;meta\u0026quot;: { \u0026quot;connectedDashboards\u0026quot;: 3, \u0026quot;created\u0026quot;: \u0026quot;2022-05-17T19:35:06Z\u0026quot;, \u0026quot;createdBy\u0026quot;: { \u0026quot;avatarUrl\u0026quot;: \u0026quot;/avatar/579fc54abdc9ab34fb4865322f2870a1\u0026quot;, \u0026quot;id\u0026quot;: 13, \u0026quot;name\u0026quot;: \u0026quot;mike.johnson@grafana.com\u0026quot; }, \u0026quot;folderName\u0026quot;: \u0026quot;mj\u0026quot;, \u0026quot;folderUid\u0026quot;: \u0026quot;R0bMCcW7z\u0026quot;, \u0026quot;updated\u0026quot;: \u0026quot;2022-05-17T19:37:14Z\u0026quot;, \u0026quot;updatedBy\u0026quot;: { \u0026quot;avatarUrl\u0026quot;: \u0026quot;/avatar/579fc54abdc9ab34fb4865322f2870a1\u0026quot;, \u0026quot;id\u0026quot;: 13, \u0026quot;name\u0026quot;: \u0026quot;mike.johnson@grafana.com\u0026quot; } }, \u0026quot;name\u0026quot;: \u0026quot;Extreme Dashboard Makeover - Grill\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;graph\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;y1C0A5unz\u0026quot;, \u0026quot;version\u0026quot;: 2 }, Please note, this is the Grill panel.
{ \u0026quot;name\u0026quot;: \u0026quot;Extreme Dashboard Makeover - Grill\u0026quot;, \u0026quot;orgId\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;graph\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;y1C0A5unz\u0026quot;, \u0026quot;version\u0026quot;: 1 } Deleting Elements # If we try to delete all the Library elements, that won\u0026rsquo;t be allowed.
./bin/gdg lib clear ERRO[0000] Failed to delete library panel titled: Dashboard Makeover - Extra Cleaning Duty Assignment Today ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Dashboard Makeover - Lighting Status ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Dashboard Makeover - Side Dish Prep Times, past 7 days ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Dashboard Makeover - Time since we purchased these spices ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Extreme Dashboard Makeover - Grill ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Extreme Dashboard Makeover - Mac Oven ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Extreme Dashboard Makeover - Refrigerator Temperature (F) ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Extreme Dashboard Makeover - Room Temperature (F) ErrorMessage=\u0026quot;the library element has connections\u0026quot; ERRO[0000] Failed to delete library panel titled: Extreme Dashboard Makeover - Salmon Cooking Times, past 7 days ErrorMessage=\u0026quot;the library element has connections\u0026quot; INFO[0000] No library were found. 0 librarys removed Deleting related dashboard # (Future version will allow you to inspect which dashboard has a link to which dashboards)
./bin/gdg dash clear -d dashboard-makeover-challenge (gke_esnet-sd-dev_us-central1-c_dev-staging-kafka-3/default) INFO[0000] 1 dashboards were deleted ┌───────────┬──────────────────────────────┐ │ TYPE │ FILENAME │ ├───────────┼──────────────────────────────┤ │ dashboard │ Dashboard Makeover Challenge │ └───────────┴──────────────────────────────┘ Please note the -d, we\u0026rsquo;re explicitly only deleting one dashboard. We can verify the list.
./bin/gdg dash list ┌────┬──────────────────────────────┬──────────────────────────────┬─────────┬───────────┬────────────────────────────────────────────────────────────────┐ │ ID │ TITLE │ SLUG │ FOLDER │ UID │ URL │ ├────┼──────────────────────────────┼──────────────────────────────┼─────────┼───────────┼────────────────────────────────────────────────────────────────┤ │ 80 │ Bandwidth Dashboard │ bandwidth-dashboard │ General │ 000000003 │ http://localhost:3000/d/000000003/bandwidth-dashboard │ │ 81 │ Bandwidth Patterns │ bandwidth-patterns │ General │ 000000004 │ http://localhost:3000/d/000000004/bandwidth-patterns │ │ 90 │ Flow Analysis │ flow-analysis │ Other │ VuuXrnPWz │ http://localhost:3000/d/VuuXrnPWz/flow-analysis │ │ 91 │ Flow Data for Circuits │ flow-data-for-circuits │ Other │ xk26IFhmk │ http://localhost:3000/d/xk26IFhmk/flow-data-for-circuits │ │ 92 │ Flow Data for Projects │ flow-data-for-projects │ Other │ ie7TeomGz │ http://localhost:3000/d/ie7TeomGz/flow-data-for-projects │ │ 93 │ Flow Data per Country │ flow-data-per-country │ Other │ fgrOzz_mk │ http://localhost:3000/d/fgrOzz_mk/flow-data-per-country │ │ 94 │ Flow Data per Organization │ flow-data-per-organization │ Other │ QfzDJKhik │ http://localhost:3000/d/QfzDJKhik/flow-data-per-organization │ │ 95 │ Flow Information │ flow-information │ Other │ nzuMyBcGk │ http://localhost:3000/d/nzuMyBcGk/flow-information │ │ 96 │ Flows by Science Discipline │ flows-by-science-discipline │ Other │ WNn1qyaiz │ http://localhost:3000/d/WNn1qyaiz/flows-by-science-discipline │ │ 83 │ Individual Flows │ individual-flows │ General │ -l3_u8nWk │ http://localhost:3000/d/-l3_u8nWk/individual-flows │ │ 82 │ Individual Flows per Country │ individual-flows-per-country │ General │ 80IVUboZk │ http://localhost:3000/d/80IVUboZk/individual-flows-per-country │ │ 88 │ Latency Patterns │ latency-patterns │ Ignored │ 000000005 │ http://localhost:3000/d/000000005/latency-patterns │ │ 84 │ Loss Patterns │ loss-patterns │ General │ 000000006 │ http://localhost:3000/d/000000006/loss-patterns │ │ 85 │ Other Flow Stats │ other-flow-stats │ General │ CJC1FFhmz │ http://localhost:3000/d/CJC1FFhmz/other-flow-stats │ │ 86 │ Science Discipline Patterns │ science-discipline-patterns │ General │ ufIS9W7Zk │ http://localhost:3000/d/ufIS9W7Zk/science-discipline-patterns │ │ 87 │ Top Talkers Over Time │ top-talkers-over-time │ General │ b35BWxAZz │ http://localhost:3000/d/b35BWxAZz/top-talkers-over-time │ └────┴──────────────────────────────┴──────────────────────────────┴─────────┴───────────┴────────────────────────────────────────────────────────────────┘ Removing related components # ./bin/gdg lib clear (gke_esnet-sd-dev_us-central1-c_dev-staging-kafka-3/default) INFO[0000] 9 library were deleted ┌─────────┬────────────────────────────────────────────────────────────────┐ │ TYPE │ FILENAME │ ├─────────┼────────────────────────────────────────────────────────────────┤ │ library │ Dashboard Makeover - Extra Cleaning Duty Assignment Today │ │ library │ Dashboard Makeover - Lighting Status │ │ library │ Dashboard Makeover - Side Dish Prep Times, past 7 days │ │ library │ Dashboard Makeover - Time since we purchased these spices │ │ library │ Extreme Dashboard Makeover - Grill │ │ library │ Extreme Dashboard Makeover - Mac Oven │ │ library │ Extreme Dashboard Makeover - Refrigerator Temperature (F) │ │ library │ Extreme Dashboard Makeover - Room Temperature (F) │ │ library │ Extreme Dashboard Makeover - Salmon Cooking Times, past 7 days │ └─────────┴────────────────────────────────────────────────────────────────┘ `}),e.add({id:5,href:"/gdg/docs/gdg/configuration/",title:"Configuration",description:`Getting started # This project requires Go to be installed. On OS X with Homebrew you can just run brew install go.
make a copy of conf/importer-example.yml and name it conf/importer.yml You\u0026rsquo;ll need GRAFANA ADMINISTRATIVE privileges to proceed.
Authentication # Authentication Token # You can use an Authentication Token / API Key to authenticate with the Grafana API, which can be generated in your Grafana Web UI =\u0026gt; Configuration =\u0026gt; API Keys.`,content:`Getting started # This project requires Go to be installed. On OS X with Homebrew you can just run brew install go.
make a copy of conf/importer-example.yml and name it conf/importer.yml You\u0026rsquo;ll need GRAFANA ADMINISTRATIVE privileges to proceed.
Authentication # Authentication Token # You can use an Authentication Token / API Key to authenticate with the Grafana API, which can be generated in your Grafana Web UI =\u0026gt; Configuration =\u0026gt; API Keys. You can then use it in your configuration file (eg. importer.yml).
👉 gdg is currently using viper to read in the config. Since viper makes all keys lowercase, we also have the same limitation. Camel case will be read in but be aware that a section named fooBar == Foobar == foobar etc. Service Accounts # Service Accounts are supported and interchangeable for tokens. If you wish to use a service account, simply put the token value from the service account for token:. Please make sure you\u0026rsquo;ve granted the account the needed permissions for the operations you are trying to perform.
context_name: main contexts: main: url: https://grafana.example.org token: \u0026quot;\u0026lt;\u0026lt;Grafana API Token\u0026gt;\u0026gt;\u0026quot; output_path: \u0026quot;myFolder\u0026quot; ignore_filters: False # When set to true all Watched filtered folders will be ignored and ALL folders will be acted on watched: - Example datasources: credentials: default: user: admin password: secret url_regex: ## set to pattern to match as well as the name. misc: user: admin password: secret url_regex: .*esproxy2* global: debug: true ignore_ssl_errors: false Username / Password # You can also use username/password credentials of an admin user to authenticate with the Grafana API. You can specify them in your configuration file (eg. importer.yml).
context_name: main contexts: main: url: https://grafana.example.org user_name: \u0026lt;your username\u0026gt; password: \u0026lt;your password\u0026gt; output_path: \u0026quot;myFolder\u0026quot; watched: - Example global: debug: true ignore_ssl_errors: false DataSource # DataSource Credentials # Current Behavior (Version +v0.4.2) # If the datasource has BasicAuth enabled, then we\u0026rsquo;ll attempt to set the auth with the following rules.
We will try to find a match given the rules specified:
field: matches any valid gjson path and retrieves it\u0026rsquo;s value. ie. jsonData.maxConcurrentShardRequests and validates it against a golang supported Regex. It goes down the list of rules and returns the auth for the first matching one. The rules should be listed from more specific to more broad. The default rule ideally should be at the end. \`\`\`yaml testing: output_path: testing_data datasources: credential_rules: - rules: - field: \u0026quot;name\u0026quot; regex: \u0026quot;misc\u0026quot; - field: \u0026quot;url\u0026quot; regex: \u0026quot;.*esproxy2*\u0026quot; auth: user: admin password: secret url: http://localhost:3000 user_name: admin password: admin ignore_filters: False # When set to true all Watched filtered folders will be ignored and ALL folders will be acted on watched: - General - Other Legacy Behavior: # If the datasource has BasicAuth enabled, then we\u0026rsquo;ll attempt to set the auth with the following precedence on matches:
Match of DS credentials based on DS name. Match URL regex for the DS if regex specified. Use Default Credentials if the above two both failed. An example of a configuration can be seen below
testing: output_path: testing_data datasources: credentials: default: user: user password: password misc: user: admin password: secret url_regex: .*esproxy2* url: http://localhost:3000 user_name: admin password: admin ignore_filters: False # When set to true all Watched filtered folders will be ignored and ALL folders will be acted on watched: - General - Other DataSource Filters # Current Behavior (+v0.4.2) # You can filter based on any field and have it be an exclusive (default) or inclusive (ie Only allow values that match) to be listed/imported etc.
Pattern matching is the same as the Credentials mapping.
field represents any valid JSON Path regex: any valid regex supported by golang The example below will exclude any datasources named \u0026ldquo;Google Sheets\u0026rdquo;. It will also only include datasources with the type elasticsearch or mysql
contexts: testing: output_path: test/data datasources: exclude_filters: - field: \u0026quot;name\u0026quot; regex: \u0026quot;Google Sheets\u0026quot; - field: \u0026quot;type\u0026quot; regex: \u0026quot;elasticsearch|mysql\u0026quot; inclusive: true Legacy Behavior # This feature allows you to exclude datasource by name or include them by type. Please note that the logic switches based on the data type.
name filter:
... datasources: filters: name_exclusions: \u0026quot;DEV-*|-Dev-*\u0026quot; Will exclude any datasource that matches the name regex.
Type Filter
Will ONLY include datasource that are listed.
datasources: filters: valid_types: - elasticsearch The snippet above will ONLY import datasources for elasticsearch
Notes # If you configure both, Auth Token and Username/Password, then the Token is given priority. Watched folders under grafana is a white list of folders that are being managed by the tool. By default only \u0026ldquo;General\u0026rdquo; is managed.
env.output defines where the files will be saved and imported from.
Global Flags # globals.debug when set will print a more verbose output (Development In Progress) globals.ignore_ssl_errors when set will disregard any SSL errors and proceed as expected
Environment Overrides # If you wish to override certain value via the environment, like credentials and such you can do so.
The pattern for GDG\u0026rsquo;s is as follows: \u0026ldquo;GDG_SECTION__SECTION__keyname\u0026rdquo;
For example if I want to set the context name to a different value I can use:
GDG_CONTEXT_NAME=\u0026quot;testing\u0026quot; gdg ctx show ## Which will override the value from the context file. GDG_CONTEXTS__TESTING__URL=\u0026quot;www.google.com\u0026quot; Will override the URL with the one provided. NOTE: Complex data type are not supported, so if the value is an array it can\u0026rsquo;t be currently set.
Building/Running the app # Running it then should be as simple as:
$ make build $ ./bin/gdg `}),e.add({id:6,href:"/gdg/docs/gdg/cloud_configuration/",title:"Cloud Configuration",description:`Cloud Support # Support for using a few cloud providers as a storage engine is now supported. When enabled, the local file system is only used for reading configuration files. Everything else relies on the data from the cloud provider matching up to your configuration.
Currently the following providers are supported:
AWS S3 Google Storage (GS) Azure 👉 https://github.com/google/go-cloud was used to support all of these providers. They should all work, but only S3 and Google have been properly tested.`,content:`Cloud Support # Support for using a few cloud providers as a storage engine is now supported. When enabled, the local file system is only used for reading configuration files. Everything else relies on the data from the cloud provider matching up to your configuration.
Currently the following providers are supported:
AWS S3 Google Storage (GS) Azure 👉 https://github.com/google/go-cloud was used to support all of these providers. They should all work, but only S3 and Google have been properly tested. Most of these rely on the system configuration. Here are some references for each respective environment:
Google Storage: https://cloud.google.com/docs/authentication#service-accounts https://cloud.google.com/docs/authentication/provide-credentials-adc#local-user-cred S3: https://docs.aws.amazon.com/sdk-for-go/api/aws/session/ Azure: https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/storage/azblob General Configuration # storage_engine: any_label: kind: cloud cloud_type: [s3, gs, azblob] bucket_name: \u0026quot;\u0026quot; prefix: \u0026quot;dummy\u0026quot; All authentication and authorization is done outside of GDG.
Context Configuration # The only additional change to the context is to provide a storage label to use:
testing: output_path: testing_data ... storage: any_label ... So given the bucket name of foo with a prefix of bar with the output_path configured as testing_data the datasources will be imported to:
s3://foo/bar/testing_data/datasources/ and exported from the same location. If you need it to be in a different location you can update the prefix accordingly but at destination will follow the typical app patterns.
`}),e.add({id:7,href:"/gdg/docs/gdg/developer/",title:"Developer Guide",description:`Running Tests # Bring up a grafana instance locally with default credentials of admin/admin. docker-compose up -d grafana Once the instance is up simply run go test ./... or make test Making a release # Install goreleaser.
brew install goreleaser/tap/goreleaser brew reinstall goreleaser\` Alternatively if you have a more recent version of Go.
go install github.com/goreleaser/goreleaser@latest export your GITHUB_TOKEN.
export GITHUB_TOKEN=\u0026quot;secret\u0026quot; git tag v0.1.0 goreleaser release
NOTE: CI/CD pipeline should do all this automatically.`,content:`Running Tests # Bring up a grafana instance locally with default credentials of admin/admin. docker-compose up -d grafana Once the instance is up simply run go test ./... or make test Making a release # Install goreleaser.
brew install goreleaser/tap/goreleaser brew reinstall goreleaser\` Alternatively if you have a more recent version of Go.
go install github.com/goreleaser/goreleaser@latest export your GITHUB_TOKEN.
export GITHUB_TOKEN=\u0026quot;secret\u0026quot; git tag v0.1.0 goreleaser release
NOTE: CI/CD pipeline should do all this automatically. make release-snapshot is used to test the release build process. Once a build is tagged all artifacts should be built automatically and attached to the github release page.
NOTE: mac binary are not signed so will likely complain.
`}),e.add({id:8,href:"/gdg/docs/gdg/examples/",title:"Example Usage",description:`Setup new configuration # You can create new context configuration using an interactive setup.
$ ./bin/gdg ctx new mycontext When creating a new context, you will be asked for authorization type, your default datasource and username/password, along with which folders you wish to manage under the context. You have three options:
Default option (\u0026ldquo;General\u0026rdquo;) List of folders you wish to manage Wildcard configuration (all folders) Import / Download Dashboards # Minimal configuration (eg.`,content:"Setup new configuration # You can create new context configuration using an interactive setup.\n$ ./bin/gdg ctx new mycontext When creating a new context, you will be asked for authorization type, your default datasource and username/password, along with which folders you wish to manage under the context. You have three options:\nDefault option (\u0026ldquo;General\u0026rdquo;) List of folders you wish to manage Wildcard configuration (all folders) Import / Download Dashboards # Minimal configuration (eg. the importer.yml file) that you need to download your dashboards from your Grafana endpoint:\ncontext_name: all contexts: all: url: https://grafana.example.org token: \u0026quot;\u0026lt;\u0026lt;Grafana API Token\u0026gt;\u0026gt;\u0026quot; # user_name: admin # password: admin output_path: exports watched: - Example - Infrastructure global: debug: true ignore_ssl_errors: false You need to adjust three parts in the configuration in order to function:\nGrafana URL: This is just a URL where your Grafana is available. API Key OR Username / Passoword for Admin user. See authentication section if you need more information. Downloaded Folders: The watched field defines folders which will be considered for manipulation. You can see these folders in your Grafana Web UI, under Dashboards \u0026gt; Management. From there, you can simply define the folders you want to be downloaded in the watched list. The dashboards are downloaded as JSON files in the $OUTPUT_PATH/dashboards/$GRAFANA_FOLDER_NAME directory. Where $OUTPUT_PATH is the path defined in the dashboard_output configuration property and $GRAFANA_FOLDER_NAME the name of the folder from which the dashboards were downloaded After you are done, and you can execute ./bin/gdg dash list successfully, eg.:\n$ ./bin/gdg dash list time=\u0026quot;2021-08-22T11:11:27+02:00\u0026quot; level=warning msg=\u0026quot;Error getting organizations: HTTP error 403: returns {\\\u0026quot;message\\\u0026quot;:\\\u0026quot;Permission denied\\\u0026quot;}\u0026quot; time=\u0026quot;2021-08-22T11:11:28+02:00\u0026quot; level=info msg=\u0026quot;Listing dashboards for context: 'all'\u0026quot; ┌────┬───────────────────────────────────┬───────────────────────────────────┬────────────────┬────────────┬────────────────────────────────────────────────────────────────────────────┐ │ ID │ TITLE │ SLUG │ FOLDER │ UID │ URL │ ├────┼───────────────────────────────────┼───────────────────────────────────┼────────────────┼────────────┼────────────────────────────────────────────────────────────────────────────┤ │ 8 │ AWS CloudWatch Logs │ aws-cloudwatch-logs │ Infrastructure │ AWSLogs00 │ https://grafana.example.org/d/AWSLogs00/aws-cloudwatch-logs │ │ 6 │ AWS ECS │ aws-ecs │ Infrastructure │ ly9Y95XWk │ https://grafana.example.org/d/ly9Y95XWk/aws-ecs │ │ 5 │ AWS ELB Application Load Balancer │ aws-elb-application-load-balancer │ Infrastructure │ bt8qGKJZz │ https://grafana.example.org/d/bt8qGKJZz/aws-elb-application-load-balancer │ │ 4 │ AWS RDS │ aws-rds │ Infrastructure │ kCDpC5uWk │ https://grafana.example.org/d/kCDpC5uWk/aws-rds │ │ 3 │ AWS S3 │ aws-s3 │ Infrastructure │ AWSS31iWk │ https://grafana.example.org/d/AWSS31iWk/aws-s3 │ │ 17 │ Cluster Autoscaling │ cluster-autoscaling │ Example │ iHUYtABMk │ https://grafana.example.org/d/iHUYtABMk/cluster-autoscaling │ └────┴───────────────────────────────────┴───────────────────────────────────┴────────────────┴────────────┴────────────────────────────────────────────────────────────────────────────┘ After executing ./bin/gdg dash import you can find the dashboards of the Infrastructure folder in the local directory dashboards/dashboards/Infrastructure and the dashboards of the Example directory in the local directory dashboards/dashboards/Example.\nExport / Upload Dashboards # Minimal configuration (eg. the importer.yml file) that you need to upload your dashboards from your Grafana endpoint:\ncontext_name: all contexts: all: url: https://grafana.example.org token: \u0026quot;\u0026lt;\u0026lt;Grafana API Token\u0026gt;\u0026gt;\u0026quot; # user_name: admin # password: admin output_path: exports watched: - Example - Infrastructure global: debug: true ignore_ssl_errors: false You need to adjust three parts in the configuration in order to function:\nGrafana URL: This is just a URL where your Grafana is available. API Key OR Username / Passoword for Admin user. See authentication section if you need more information. Uploaded Folders: The watched field defines folders which will be considered for manipulation. The dashboards should be stored as JSON files in the $OUTPUT_PATH/dashboards/$GRAFANA_FOLDER_NAME directory. Where $OUTPUT_PATH is the path defined in the dashboard_output configuration property and $GRAFANA_FOLDER_NAME the name of the folder to which the dashboards will be uploaded. In case of the above configuration file, the dashboards should be stored locally in the dashboards/dashboards/Example and dashboards/dashboards/Infrastructure/ directories. ├── bin | └── gdg └── exports └── dashboards ├── Example | └── cluster-scaling.json └── Infrastructure └── aws-ecs.json You can execute ./bin/gdg dash export to upload the local dashboards to your Grafana. Afterwards, you can try running ./bin/gdg dash list in order to confirm that your dashboards were uploaded successfully.\n"}),e.add({id:9,href:"/gdg/docs/gdg/usage_guide/",title:"Usage Guide",description:`Every namespace supporting CRUD operations has the functions: list, download, upload, clear operating on only the monitored folders.
Authentication Management # This is mainly added as a convenience mechanism. It was needed to support some testing and exposing the feature is useful as a really simple CLI to create tokens / service Keys. You probably should be using other tooling for managing all your service files and tokens. Unlike most other entities, this is not a backup feature as much as utility.`,content:`Every namespace supporting CRUD operations has the functions: list, download, upload, clear operating on only the monitored folders.
Authentication Management # This is mainly added as a convenience mechanism. It was needed to support some testing and exposing the feature is useful as a really simple CLI to create tokens / service Keys. You probably should be using other tooling for managing all your service files and tokens. Unlike most other entities, this is not a backup feature as much as utility.
There are two sub commands for auth, service-accounts and tokens (will be deprecated at some point).
Token Management # ./bin/gdg auth tokens list -- list current tokens (No access to the actual token secret) ./bin/gdg auth tokens new -- Create a new token. new \u0026lt;name\u0026gt; \u0026lt;role\u0026gt; [ttl in seconds, forever otherwise] ./bin/gdg auth tokens clear -- Deletes all tokens Token Listing ┌────┬─────────┬───────┬───────────────┐ │ ID │ NAME │ ROLE │ EXPIRATION │ ├────┼─────────┼───────┼───────────────┤ │ 1 │ testing │ Admin │ No Expiration │ └────┴─────────┴───────┴───────────────┘ Example of creating a new token.
./bin/gdg auth tokens new foobar Admin 3600 New Token ┌────┬────────┬─────────────────────────────────────────────────────────────┐ │ ID │ NAME │ TOKEN │ ├────┼────────┼─────────────────────────────────────────────────────────────┤ │ 2 │ foobar │ eyJrIjoiNzU2WVhiMEZpVWNlV3hWSUVZQTuIjoiZm9vYmFyIiwiaWQiOjF9 │ └────┴────────┴─────────────────────────────────────────────────────────────┘ Service Accounts # ./bin/gdg svc clear delete all Service Accounts ./bin/gdg svc clearTokens delete all tokens for Service Account ./bin/gdg svc list list API Keys ./bin/gdg svc newService newService \u0026lt;serviceName\u0026gt; \u0026lt;role\u0026gt; [ttl in seconds] ./bin/gdg svc newToken newToken \u0026lt;serviceAccountID\u0026gt; \u0026lt;name\u0026gt; [ttl in seconds] ./bin/gdg auth svc newService AwesomeSauceSvc admin New Service ┌────┬─────────────────┬───────┐ │ ID │ NAME │ ROLE │ ├────┼─────────────────┼───────┤ │ 4 │ AwesomeSauceSvc │ Admin │ └────┴─────────────────┴───────┘ ./bin/gdg auth svc newToken 4 AwesomeToken New Service ┌───────────┬──────────┬──────────────┬────────────────────────────────────────────────┐ │ SERVICEID │ TOKEN_ID │ NAME │ TOKEN │ ├───────────┼──────────┼──────────────┼────────────────────────────────────────────────┤ │ 4 │ 3 │ AwesomeToken │ glsa_a14JOaGExOkDuJHjDWScXbxjTBIXScsw_39df7bf5 │ └───────────┴──────────┴──────────────┴────────────────────────────────────────────────┘ ./bin/gdg auth svc list Service Listing ┌────┬─────────────────┬───────┬────────┬──────────┬──────────────┬───────────────┐ │ ID │ SERVICE NAME │ ROLE │ TOKENS │ TOKEN ID │ TOKEN NAME │ EXPIRATION │ ├────┼─────────────────┼───────┼────────┼──────────┼──────────────┼───────────────┤ │ 4 │ AwesomeSauceSvc │ Admin │ 1 │ │ │ │ │ │ │ │ │ 3 │ AwesomeToken │ No Expiration │ └────┴─────────────────┴───────┴────────┴──────────┴──────────────┴───────────────┘ Alert Notifications (DEPRECATED) # This will stop working soon both as a concept in grafana and something that GDG will support.
Allows you to manage alertnotifications (an) if you have any setup
./bin/gdg an list -- Lists all alert notifications ./bin/gdg an download -- retrieve and save all alertnotifications from grafana ./bin/gdg an upload -- writes all local alert notifications to grafana ./bin/gdg an clear -- Deletes all alert notifications Contexts # Starting with version 0.1.4 contexts are now supported. Your config can contain one or multiple contexts which are essentially a grafana server configuration.
ctx is shorthand for context and basic CRUD is supported which is mainly tooling to make it easier to avoid updating the yaml file manually
./bin/gdg ctx list -- Lists all known contexts ./bin/gdg ctx show qa -- shows the configuration for the selected context ./bin/gdg ctx set production -- updates the active config and sets it to the request value. ./bin/gdg ctx delete qa -- Deletes the QA context ./bin/gdg ctx cp qa staging -- copies the qa context to staging and sets it as active ./bin/gdg ctx clear -- Will delete all active contexts leaving only a single example entry Dashboards # Dashboards are imported or exported from organization specified in configuration file otherwise current organization user is used.
All commands can use dashboards or dash to manage dashboards
./bin/gdg dash list -- Lists all current dashboards ./bin/gdg dash download -- Import all dashboards from grafana to local file system ./bin/gdg dash upload -- Exports all dashboard from local filesystem (matching folder filter) to Grafana ./bin/gdg dash clear -- Deletes all dashboards You can also use filtering options to list or import your dashboard by folder or by tags.
./bin/gdg dash download -f myFolder ./bin/gdg dash download -t myTag ./bin/gdg dash download -t tagA -t tagB -t tagC DataSources # DataSources credentials are keyed by the name of the DataSource. See see config example. If the datasource JSON doesn\u0026rsquo;t have auth enabled, the credentials are ignored. If Credentials are missing, we\u0026rsquo;ll fall back on default credentials if any exist. The password is set as a value for basicAuthPassword in the API payload. Datasources are imported or exported from organization specified in configuration file otherwise current organization user is used.
All commands can use datasources or ds to manage datasources
./bin/gdg ds list -- Lists all current datasources ./bin/gdg ds download -- Import all datasources from grafana to local file system ./bin/gdg ds upload -- Exports all dashboard from local filesystem (matching folder filter) to Grafana ./bin/gdg ds clear -- Deletes all datasources Devel # Some developer helper utilities
./bin/gdg devel completion [bash|fish|powershell|zsh] -- Will generate autocompletion for GDG for your favorite shell ./bin/gdg devel srvinfo -- print grafana server info Folders # Mostly optional as Dashboards will create/delete these are needed but if there is additional metadata you wish to persist you can use this to manage them.
./bin/gdg folders list -- Lists all current folders ./bin/gdg folders download -- Import all folders from grafana to local file system ./bin/gdg folders upload -- Exports all folders from local filesystem ./bin/gdg folders clear -- Deletes all folders Library Elements # Library elements are components that can be shared among multiple dashboards. Folder matching will still be applied, so any folders not monitored will be ignored unless explicitly specified. If wildcard flag is enabled, all elements will be acted on irrelevant of folder location
All commands can use libraryelements aliased to library and lib for laziness purposes.
./bin/gdg lib list -- Lists all library components ./bin/gdg lib download -- Import all library components from grafana to local file system ./bin/gdg lib upload -- Exports all library components from local filesystem (matching folder filter) to Grafana ./bin/gdg lib clear -- Deletes all library components ./bin/gdg lib list-connections \u0026lt;Lib Element UID\u0026gt; -- Will list all of the dashboards connected to the Lib Element (Coming in v0.4.2) Organizations # Command can use organizations or org to manage organizations.
./bin/gdg org list -- Lists all organizations Teams # 👉 Admin team members are unable to be exported back. Currently all members except the server admin will be exported as regular members 👉 Users need to be created before team export can succeed ./bin/gdg team list -- Lists all known team members ./bin/gdg team download -- download all known team members ./bin/gdg team upload -- upload all known team members ./bin/gdg team clear -- Delete all known team except admin Team Listing ┌────┬───────────┬───────┬───────┬─────────┬─────────────┬──────────────┬───────────────────┐ │ ID │ NAME │ EMAIL │ ORGID │ CREATED │ MEMBERCOUNT │ MEMBER LOGIN │ MEMBER PERMISSION │ ├────┼───────────┼───────┼───────┼─────────┼─────────────┼──────────────┼───────────────────┤ │ 4 │ engineers │ │ 1 │ 2 │ │ │ │ │ │ │ │ │ │ admin │ Admin │ │ │ │ │ │ │ │ tux │ Member │ │ │ 5 │ musicians │ │ 1 │ 1 │ │ │ │ │ │ │ │ │ │ admin │ Admin │ │ └────┴───────────┴───────┴───────┴─────────┴─────────────┴──────────────┴───────────────────┘ Users # Only supported with basic auth. Users is the only one where basic auth is given priority. API Auth is not supported, so will try to use basic auth if configured otherwise will warn the user and exit.
NOTE: admin user is always ignored.
./bin/gdg users list -- Lists all known users ./bin/gdg users promote -u user@foobar.com -- promotes the user to a grafana admin ./bin/gdg users download -- Lists all known users ./bin/gdg users upload -- Export all users (Not yet supported) ./bin/gdg users clear -- Delete all known users except admin Token Management (available on +v0.4.2) # Allows the user to create, delete and list current tokens.
./bin/gdg tokens list -- Lists all known tokens ./bin/gdg tokens clear -- Delete all known tokens ./bin/gdg tokens new \u0026lt;token_name\u0026gt; \u0026lt;role\u0026gt; [ttl] -- Delete all tokens Creation:
./bin/gdg tokens new foobar viewer 31536000 New Key ┌────┬────────┬──────────────────────────────────────────────────────────────────────────────────┐ │ ID │ NAME │ TOKEN │ ├────┼────────┼──────────────────────────────────────────────────────────────────────────────────┤ │ 15 │ foobar │ eyJrIjoiQXlQY0xMTDg1N09xZkZtc3VGck1iM01YNVNyaHZwTDkiLCJuIjoiZm9vYmFyIiwiaWQiOjF9 │ └────┴────────┴────────────────────────────────────────────────────────────────────────────────── ./bin/gdg tokens list Listing ┌────┬────────┬────────┬──────────────────────────┐ │ ID │ NAME │ ROLE │ EXPIRATION │ ├────┼────────┼────────┼──────────────────────────┤ │ 12 │ me │ Admin │ No Expiration │ │ 13 │ booh │ Viewer │ No Expiration │ │ 14 │ moo │ Editor │ No Expiration │ │ 15 │ foobar │ Viewer │ 2024-04-03T18:08:57.000Z │ └────┴────────┴────────┴──────────────────────────┘ Version # Print the applications release version
./bin/gdg version Build Date: 2022-05-05-13:27:08 Git Commit: 34cc84b3d80080aa93e74ed37739bddc3638997c+CHANGES Version: 0.1.11 Go Version: go1.18 OS / Arch: darwin amd64 `}),e.add({id:10,href:"/gdg/docs/releases/gdg_0.4/",title:"Release notes v0.4.X",description:"Release Notes for v0.4",content:`Release Notes for v0.4.3 # Release Date: 04/14/2023
New Features # Team CRUD support, allows full CRUD on all team and members. Fixes #127 and #147 Known Bug: Permissioning not persisted. All users are added as a member. See issue 149 CLI Tooling introduced to faciliate very basic service management, and token creations for both services and API tokens. Improved Credential mapping and filtering introduced. Allows filtering and credential mapping to be based on any JSON field and regex. Configuration Changes # DataSource has had a configuration overhaul. It is technically backward compatible, all previous tests work, with the previous config, but I would highly encourage people to migrate. Next feature I will drop the backward support. URLMatching for Credentials will not work (legacy pattern) if the URL AND the datasource do not match. If you need URL matching with variable datasource names, you will need to migrate to the new configuration. Release Notes for v0.4.2 # Issue with release, failed CI, so skipping version.
Release Notes for v0.4.1 # Release Date: 04/01/2023
New Features # Library Elements Connections # Added support for libraryelement connections. This option allows you to see what dashboards are using the given library. note: You won\u0026rsquo;t be able to delete the library element while there are dashboards utilizing it. Bug Fixes # FIXED: Addressing Login issue when Basic Auth is omitted. #144 Release Notes for v0.4.0 # Release Date: 03/31/2023
This is a major change from the previous one, I\u0026rsquo;ll likely cut the 1.x soon and start following the more typical Semver conventions. Aka Major version is a breaking change, Minor is just that, patches for previous versions.
Please see the API Changes notes below.
New Features # Wild card flag # You can now set a flag under each context that will ignore Watched Folders and retrieve all dashboards.
context_name: filter_override: ignore_dashboard_filters: false # LibraryElements support added. # Please see the usage guide here and a brief tutorial available here
Folders Update # Introducing a \u0026ndash;use-filters. When enabled will only operate on folders configured. Default is to create/update/delete all folders in the grafana instance.
Breaking Changes: # SFTP support dropped. # See the Cloud configuration section. Switched out the library we relied on, which means the auth has moved out of GDG config and relies on the system config.
API SDK Changes: # I have been trying to find a proper library to use so I\u0026rsquo;m not re-writing and reinventing the wheel so to speak.
For reference, here are all the current \u0026ldquo;active\u0026rdquo; (active can be a relative term for some of these project) development I\u0026rsquo;m aware of.
Grafana Tools SDK Initial version of GDG was based on this project. It mostly works but getting any PRS accepted can be tedious and it\u0026rsquo;s needs some help. Grafana API Go Client Owned by the Grafana Org which is nice, but it has a slightly different goal. It\u0026rsquo;s primary goal is to support the terraform provider for Grafana. I also found some endpoints missing very early on. So decided not to go with it. Swagger Based: There\u0026rsquo;s a branch that I\u0026rsquo;ve been keeping an eye on. https://github.com/grafana/grafana-api-golang-client/tree/papagian/generate-client-from-swagger which makes an effort to generate code based on the swagger manifest that\u0026rsquo;s available from Grafana. It\u0026rsquo;s a mostly automated code that pulls data from the Schema and generates the underlying code. It hasn\u0026rsquo;t had much traction of late so I ended up forking the project currently available here Final Choice: # Although the Swagger/OpenAPI based version is not great, I\u0026rsquo;ve even ran into a few issues where the documented response does not match the result, it\u0026rsquo;s a lot more encompassing and allows further development without being as limited on upstream changes.
DataModel Changes # I\u0026rsquo;ve tried to utilize mostly the same endpoints to recreate the same behavior for all the various entities, but there is are some changes. For most use cases this shouldn\u0026rsquo;t matter. But you have been officially warned.
Cloud Support # The previous abstraction library used to provide S3, GS, SFTP has limited activity and introduced some security vulnerabilities. 0.4.X also changes some of the cloud behavior. It relies on the system authentication rather than having the auth in the config file.
Please see the related docs on how to configure your environment.
As the Stow library was removed, SFTP has been dropped. The list of current supported cloud providers are: S3, GS, Azure.
`}),e.add({id:11,href:"/gdg/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()